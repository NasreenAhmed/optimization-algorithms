{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inexact_line_search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyLuekNlAYns"
      },
      "source": [
        "# Approximate Line Search\n",
        " If is often more computationally effecient to perform more iterations of a descent method than to do exact line search at each iteration, espicially if the function and derivative calculations are expensieve. Many of the methods discussed so far ( exact line search ) can benefit from using **approximate line search** to find a suitable step size with a small number of evaluations. Since descent methods must descent, a step size $\\alpha$ may be suitable if it causes a decrease in the objective function value. However, a variety of other conditions may be enforced to encourage faster convergence. The condtion for *sufficient decrease* requires that the step size cause a sufficient decrease in the objective function value:\\\n",
        " $f(x^{(k+1)} \\leq  f(x^{(k)} + \\beta\\alpha \\nabla_{d^{(k)}} f(x^{(k)})$\n",
        "\n",
        " with $\\beta \\in [0,1]$ often set to $\\beta = 1 * 10^{-4}.$ If $\\beta=0$, then any decrease is acceptable. If $beta = 1$, then the decrease has to be at least as much as what would be predicted by a first-order approximation.\n",
        "\n",
        " If $d$ is a valid descent direction, then there must exist a sufficiently small step size that satisfies the sufficient decrease condition. We thus start with a large step size and decrease it by a constant reduction factor until the sufficient decrease condition is satisfied. This algorithm is known as *backtracking line search* because of how it backtracks along the descent direction. \n",
        "\n",
        "```\n",
        " function backtracking_line_search(f,del_f,x,d, alpha; p=0.5, beta = 1e-4)\n",
        "  y,g = f(x), del_f(x)\n",
        "  while f(x+alpha*d) > y + beta*alpha*(g.d)\n",
        "     alpha *= p\n",
        "  end\n",
        "  alpha\n",
        "end\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr8trqQ1DU7b"
      },
      "source": [
        "Q. Consider approximate line search on $f(x_1,x_2) = x_1^2+x_1x_2+x_2^2$ from $x=[1,2]$ in the direction $d=[-1,-1]$, using the maximum step size of 10, a reduction factor of 0.5, first Wolfe condition $\\beta = 1 * 10^{-4}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsUc2F3iKn_V"
      },
      "source": [
        "# Same function used as function itself and its derivative, based on the value of power.\n",
        "def f(x,a):\n",
        "  pow = a\n",
        "  if pow == 0:\n",
        "   return x[0]**2 + x[0]*x[1] + x[1]**2\n",
        "  else:\n",
        "    return [x[0]*2+x[1],x[1]*2+x[0]]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_IwvTJudqEs"
      },
      "source": [
        "def backtracking_line_search(x,d,alpha,p=0.5,beta = 1e-4,sigma =0.9):\n",
        "  y, dy = f(x,0), f(x,1)\n",
        "  arr = [alpha]\n",
        "  # Execute the while loop until the first Wolfe condition is satisfied\n",
        "  while f(x+alpha*d,0) > y +  beta*alpha*np.dot(dy,d):\n",
        "    alpha*=p\n",
        "    arr.append(alpha)\n",
        "  return [arr,alpha]\n",
        " "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZwAOLLqLpGB",
        "outputId": "132b7b8b-3af6-42e6-9b26-1d3854fdde65"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array([1,2])\n",
        "d = np.array([-1,-1])\n",
        "alpha = 10\n",
        "backtracking_line_search(x,d,alpha)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[10, 5.0, 2.5], 2.5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    }
  ]
}